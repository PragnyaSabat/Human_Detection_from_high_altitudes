# Human_Detection_from_high_altitudes
Lightweight small-human detection system for high-altitude drone imagery using YOLO + SAHI. Optimized with pruning, quantization, and TFLite conversion for real-time edge deployment. Designed for efficient aerial surveillance and precise payload-target identification on Android devices.

## Mobile-Optimized Human Detection (SAHI + YOLO)
README Documentation (Deployment-Oriented Version)

This document explains:

âœ… A mobile-optimized architecture

ğŸ“Š Complete inference time increase analysis

âš™ï¸ Memory and latency trade-offs

ğŸš€ Recommended Android deployment pipeline

This is written specifically for high-altitude small human detection use cases.

### 1ï¸âƒ£ Why Your Current Multi-Model + SAHI Is NOT Mobile-Friendly

Your current system:

YOLOv10n

YOLOv9t

YOLOv8n

SAHI slicing

IoU fusion

Computational Impact

Let:

Single YOLO inference = T

Number of models = 3

SAHI slices per image = S

Total inference â‰ˆ 3 Ã— S Ã— T

If:

T = 25 ms (YOLOv10n GPU)

S = 6 slices (1920Ã—1080 with overlap)

Then:

Total â‰ˆ 3 Ã— 6 Ã— 25 ms
      â‰ˆ 450 ms

ğŸ‘‰ ~0.45 seconds per image

On CPU/mobile â†’ much slower (1â€“2 seconds).

Memory usage:

YOLOv10n â‰ˆ 3â€“4 MB

YOLOv9t â‰ˆ 5â€“6 MB

YOLOv8n â‰ˆ 3â€“4 MB

Total model memory â‰ˆ 12â€“14 MB (before runtime buffers)

On Android with TFLite:

RAM usage increases further due to tensors & buffers.

### 2ï¸âƒ£ Mobile-Optimized Approach (Recommended Architecture)

Instead of 3 models + fusion, use:

âœ… Single NAS-Optimized Model + SAHI (Lightweight Mode)

Architecture:

Input Image
      â†“
Adaptive SAHI (small slice count)
      â†“
Single Optimized YOLO (Pruned + Quantized)
      â†“
Human Class Filtering
      â†“
Output
### 3ï¸âƒ£ Optimization Strategy
Step 1 â€” Use Only One Model

Recommended base:

YOLOv10n (smallest, efficient backbone)

Remove:

Multi-model ensemble

Late fusion overhead

Step 2 â€” Structured Pruning

Remove low-importance channels.

Typical reduction:

20â€“40% parameters

10â€“25% latency reduction

Step 3 â€” INT8 Quantization

Convert to:

ONNX â†’ TFLite

Full INT8 quantization

Benefits:

4Ã— smaller weights

30â€“50% faster inference on mobile CPU

Step 4 â€” Adaptive SAHI

Instead of fixed slicing:

Use slicing only if image resolution > threshold.

Example logic:

If image_width > 1500:
    Apply SAHI
Else:
    Run direct inference

This reduces unnecessary slicing overhead.

### 4ï¸âƒ£ Inference Time Comparison (Complete Breakdown)

Assume 1920Ã—1080 drone frame.

Setup	Models	SAHI	Approx Time
Single YOLO	1	No	25â€“40 ms
Single YOLO + SAHI	1	Yes	120â€“180 ms
Multi-Model (3)	3	No	75â€“120 ms
Multi-Model + SAHI	3	Yes	400â€“600 ms
Optimized (Pruned + INT8 + Adaptive SAHI)	1	Conditional	60â€“120 ms
### 5ï¸âƒ£ Complete Inference Time Increase Explanation

Let:

Base single model time = T

Number of models = M

Number of slices = S

Then:

Total Time â‰ˆ M Ã— S Ã— T

For your original pipeline:

M = 3

S â‰ˆ 6â€“8

T â‰ˆ 30 ms

Total â‰ˆ 540â€“720 ms

That is:

15â€“20Ã— slower than a single direct YOLO inference.

### 6ï¸âƒ£ Recommended Mobile Architecture (Final)
Lightweight Android-Ready Pipeline
Drone Frame
      â†“
Resolution Check
      â†“
Conditional SAHI
      â†“
Single Pruned YOLOv10n (INT8 TFLite)
      â†“
Human Filtering
      â†“
Bounding Boxes
### 7ï¸âƒ£ Expected Mobile Performance

On mid-range Android device:

Stage	Time
Preprocessing	10 ms
TFLite INT8 Model	40â€“70 ms
SAHI (if enabled)	+40 ms
Total	80â€“120 ms

That enables:

âœ” ~8â€“12 FPS
âœ” Real-time capable
âœ” Battery efficient

### 8ï¸âƒ£ Why This Is Better Than Multi-Model
Feature	Multi-Model	Optimized Single
Recall	High	High (after tuning)
Latency	Very High	Moderate
Memory	High	Low
Edge Deployable	No	Yes
Battery Friendly	No	Yes
### 9ï¸âƒ£ Research vs Deployment Mode
Research Mode

Use:

Multi-model + SAHI

Maximum recall

Benchmarking

Deployment Mode

Use:

NAS-optimized YOLO

Pruning

Quantization

Adaptive SAHI

### ğŸ”Ÿ Final Recommendation for Your Project

Since your goal involves:

Drone-based human detection

Android deployment

Edge efficiency

The correct production strategy is:

Distill multi-model knowledge into one optimized student model, then prune + quantize + apply adaptive SAHI.

This keeps:

âœ” High small-human recall
âœ” Low latency
âœ” Efficient memory usage
âœ” Practical mobile deployment

## ğŸ“Œ Conclusion

Your original Multi-Model + SAHI system:

Maximizes detection robustness

Increases inference time by ~15â€“20Ã—

Not mobile efficient

The optimized single-model pipeline:

Maintains strong accuracy

Reduces latency by ~70â€“85%

Suitable for real-time Android deployment
